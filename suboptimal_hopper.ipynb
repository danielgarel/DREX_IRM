{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very useful website\n",
    "https://github.com/openai/gym/blob/master/gym/envs/mujoco/hopper_v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import pickle\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO as PPOSB\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sb_args(sb_args):\n",
    "    def without_keys(d, *keys):\n",
    "        return dict(filter(lambda key_value: key_value[0] not in keys, d.items()))\n",
    "\n",
    "    sb_args = without_keys(sb_args, 'n_envs', 'n_timesteps', 'policy',\n",
    "                           'env_wrapper', 'normalize')\n",
    "\n",
    "    # # process policy_kwargs str\n",
    "    if 'policy_kwargs' in sb_args.keys():\n",
    "        if isinstance(sb_args['policy_kwargs'], str):\n",
    "            sb_args['policy_kwargs'] = eval(sb_args['policy_kwargs'])\n",
    "\n",
    "    # process schedules\n",
    "    for key in [\"learning_rate\", \"clip_range\", \"clip_range_vf\"]:\n",
    "        if key not in sb_args:\n",
    "            continue\n",
    "        if isinstance(sb_args[key], str):\n",
    "            schedule, initial_value = sb_args[key].split(\"_\")\n",
    "            initial_value = float(initial_value)\n",
    "            sb_args[key] = linear_schedule(initial_value)\n",
    "\n",
    "    return sb_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name_string(name_string):\n",
    "    name_string = name_string.replace('{', '_').replace('}', '').replace(' ', '').replace(\"'xml_file'\", '')\n",
    "    name_string = name_string.replace(\"'\", \"\").replace(\":\", \"\").replace('/', '')\n",
    "    name_string = name_string.replace(\".xml\", \"\")\n",
    "\n",
    "\n",
    "    return name_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [227.27147] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [81.61481] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [118.528] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [545.5633] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [401.7378] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [319.3967] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [381.11465] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [379.89987] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [720.11816] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [803.03424] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [758.0456] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.63904] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.84204] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [976.41956] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.1868] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.6234] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [985.7177] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [937.22473] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.0748] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.4032] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.26697] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.7913] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [965.2815] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.08606] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.0359] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.90717] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.59216] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.2575] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [939.25757] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [916.65076] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [924.9783] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [914.88794] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [889.64557] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [742.4496] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [909.5861] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [900.14764] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [922.20435] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [914.63245] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [937.8751] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [900.258] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [919.93353] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.64075] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [733.78125] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [967.09924] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.12787] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.7429] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [850.63513] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.6038] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [977.0587] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.5985] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.2244] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.83875] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.57733] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [971.06085] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.5567] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.833] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.46655] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [945.9109] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [939.1692] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.60724] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.2134] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [937.5109] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [924.3812] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.5684] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.4521] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [924.0767] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [895.8013] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [929.65594] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [922.96674] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [899.8792] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [887.7609] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [858.4396] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [892.7708] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [842.4937] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [825.1859] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [799.6941] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [760.0705] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [741.59283] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [723.36743] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [701.0132] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [702.85156] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [774.12445] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [806.79474] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [803.91864] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [822.1362] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [837.7267] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [840.5564] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [831.9104] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [818.7597] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [824.5857] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [848.145] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [891.4247] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [891.8973] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [866.1913] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [868.94086] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [898.6235] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [902.0845] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [888.89246] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [898.7617] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [914.11273] extra threshold 0\n",
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [295.7005] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [219.32036] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [833.363] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [321.3986] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [432.50574] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [260.06003] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: [479.2807] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [449.62045] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [237.58284] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [469.2391] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [402.76288] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.25616] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [512.26996] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [941.2764] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.74243] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [737.0246] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.3385] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [924.81116] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [928.16504] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [909.9662] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.0115] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [928.2354] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.9135] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [643.298] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [900.7821] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [923.85] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.2692] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [907.1398] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.965] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.03534] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.1658] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [660.69104] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.9875] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.86285] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.5528] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [668.0452] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [970.20856] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [963.6334] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.9306] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.25665] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.31537] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.5828] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.9917] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.82874] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [941.5279] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.4413] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.77466] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.6899] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.7425] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.5834] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.2591] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.5016] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.01337] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.4813] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.0787] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.902] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.47675] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [928.65924] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [912.7935] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [907.87823] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [922.9759] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [894.1768] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [912.61096] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.9721] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [907.6346] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [892.2903] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [868.26276] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [875.05493] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [865.11395] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [883.9038] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [867.19257] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [877.2644] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [849.587] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [869.514] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [834.3661] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [871.83105] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [844.6376] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [852.315] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [829.5853] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [889.3152] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [912.23706] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [917.9157] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [907.01447] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [918.1725] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [884.71356] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [877.6476] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [877.5432] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [885.0241] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [865.9352] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [837.6016] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [803.1579] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [773.7048] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [795.23254] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [798.78186] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [793.7886] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [806.65283] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [835.65967] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [838.1038] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [821.77014] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [783.6891] extra threshold 0\n",
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [197.69102] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [256.44507] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [307.2344] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [205.82668] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [342.29776] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [372.99527] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [728.9563] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [167.81963] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [377.58304] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [328.98517] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [926.07715] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.808] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.0164] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: [976.0513] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [971.9883] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [720.2897] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [339.70554] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.1582] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [691.7419] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [433.64554] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.0156] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.96265] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.7586] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.21136] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [978.74347] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.67834] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.71234] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.7926] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [945.56683] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.69763] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [962.3296] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.81915] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [977.9102] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.18933] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [937.15924] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [983.5783] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [980.4899] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.8455] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.15027] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [962.25507] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [974.4507] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.3317] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [979.15955] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.62573] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [923.03864] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [901.13043] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [888.19635] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [918.2301] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [931.69745] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.22815] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.64] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.4254] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [961.0257] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [977.75055] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [970.522] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.2715] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [967.145] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [970.298] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.5811] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.8922] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.71234] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.9164] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.20984] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.38416] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.0386] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [979.4708] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.3515] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.7706] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [683.0496] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [974.30475] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.9299] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.94653] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.9369] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [418.3769] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [973.0573] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.1333] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [979.2426] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [978.8892] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [975.5239] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [976.7381] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.22577] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [987.2211] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [971.6536] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [975.01544] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [975.58496] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [968.25134] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.93097] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [963.61957] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [965.81354] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [963.3232] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.0509] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.17505] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.8579] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.6098] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [938.49243] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.5054] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.76447] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.0921] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.90204] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.9115] extra threshold 0\n",
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [188.76965] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [314.5971] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [160.02722] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [159.18658] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [842.77106] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [555.8155] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [282.1371] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [430.78925] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [440.6687] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [412.03488] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [593.3796] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.73883] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [915.6433] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [878.34283] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.25024] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.78925] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [917.4549] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [903.6584] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.0879] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: [920.86084] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.81024] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [802.1718] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.66626] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.6983] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [931.73926] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.9328] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [939.7039] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [892.6904] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [911.4598] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [899.2424] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [907.81824] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [891.1485] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [937.83264] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [875.57947] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [913.0793] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [878.60614] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [887.63165] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [891.428] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [931.27936] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [860.91754] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [879.91815] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [920.66254] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [882.406] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [896.0975] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [898.16406] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [919.588] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.587] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.4112] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.49884] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.31165] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [928.6] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.68646] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [938.3174] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [938.05194] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.0324] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [945.7254] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [917.99615] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [900.44714] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [871.08093] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [874.012] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [815.28845] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [853.9702] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [826.4776] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [846.94476] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [849.2073] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [868.8076] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [886.58234] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [880.12994] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [877.24347] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [870.7623] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [893.4938] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [918.9769] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [901.2183] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [909.9493] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [910.3163] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [892.34644] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [901.5925] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [909.53876] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [813.3739] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [828.8433] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [808.9906] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [857.5308] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [839.2979] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [859.9399] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [826.3334] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [751.80035] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [788.8166] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [810.214] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [804.2943] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [778.294] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [799.0948] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [780.3325] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [763.13446] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [767.26227] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [833.34595] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [845.8266] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [851.21655] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [846.2598] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [807.1003] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [789.0222] extra threshold 0\n"
     ]
    }
   ],
   "source": [
    "for spec, costs in zip(env_kwargs, [[0.01, 0.008], [0.1, 0.17]]):\n",
    "    for cost in costs:\n",
    "        model_dir = './sb_models/ckpt_' + env_name + format_name_string(str(spec)) + '_' + str(cost)\n",
    "\n",
    "        for model_file in os.listdir(model_dir):\n",
    "            model = PPOSB.load(os.path.join(model_dir, model_file))\n",
    "            save_expert_traj(env, model, spec,cost, pl_model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_expert_traj(env, model, spec_kwargs, cost, extra_reward_threshold=0,\n",
    "                     nr_trajectories=1, pl_model_file=None):\n",
    "    num_steps = 0\n",
    "    expert_traj = []\n",
    "    expert_traj_extra = []\n",
    "\n",
    "    if pl_model_file is not None:\n",
    "        print(pl_model_file)\n",
    "     \n",
    "    \n",
    "    for i_episode in count():\n",
    "        print(i_episode)\n",
    "        ob = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_traj = []\n",
    "        stacked_vec = []\n",
    "        \n",
    "        # Adding the lines below to match the dataset.pkl from CORL - DG:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        while not done:\n",
    "            ac, _states = model.predict(ob)\n",
    "            # print(env)\n",
    "            # if not isinstance(ac,list):\n",
    "            # ac = np.array([ac])\n",
    "            next_ob, reward, done, _ = env.step(ac)\n",
    "            ob = next_ob\n",
    "            total_reward += reward\n",
    "            # if len(ob.shape) != len(ac.shape):\n",
    "            # print(\"shape mismatch\")\n",
    "            # ob = np.squeeze(ob)\n",
    "            observations.append(np.squeeze(np.array(ob)))\n",
    "            actions.append(np.squeeze(np.array(ac)))\n",
    "            rewards.extend(reward)\n",
    "#             stacked_vec = np.hstack([np.squeeze(ob), np.squeeze(ac), reward, done])\n",
    "#             stacked_vec = (np.concatenate(ob,axis=0),np.concatenate(ac,axis=0),np.concatenate(reward,axis=0))\n",
    "#             expert_traj.append(stacked_vec)\n",
    "#             episode_traj.append(stacked_vec)\n",
    "            num_steps += 1\n",
    "        \n",
    "        stacked_vec.append(np.array(observations))\n",
    "        stacked_vec.append(np.array(actions))\n",
    "        stacked_vec.append(rewards)\n",
    "        expert_traj.append(stacked_vec)\n",
    "#         episode_traj.append(stacked_vec)\n",
    "        \n",
    "        \n",
    "        print(\"episode:\", i_episode, \"reward:\", total_reward,\n",
    "              \"extra threshold\", extra_reward_threshold)\n",
    "\n",
    "#         if total_reward > extra_reward_threshold:\n",
    "#             expert_traj_extra.extend(episode_traj)\n",
    "\n",
    "        if i_episode == nr_trajectories-1:\n",
    "            break\n",
    "\n",
    "    filename = env_name + format_name_string(str(spec_kwargs)) +str(cost)\n",
    "    if pl_model_file is not None:\n",
    "        filename = filename + pl_model_file\n",
    "    demo_dir = './demos'\n",
    "    if not os.path.exists(demo_dir):\n",
    "        os.mkdir(demo_dir)\n",
    "        os.mkdir(os.path.join(demo_dir, 'preference_learning'))\n",
    "        os.mkdir(os.path.join(demo_dir, 'preference_learning', env_name + spec_kwargs + str(cost)))\n",
    "\n",
    "#     expert_traj = np.stack(expert_traj)\n",
    "\n",
    "    if pl_model_file is not None:\n",
    "        with open(os.path.join(demo_dir, 'preference_learning/' + filename + \"_expert_traj.pkl\"), 'wb') as f:\n",
    "            pickle.dump(expert_traj, f)\n",
    "    else:\n",
    "        with open(os.path.join(demo_dir, filename + \"_expert_traj.pkl\"), 'wb') as f:\n",
    "            pickle.dump(expert_traj, f)\n",
    "        # np.save(os.path.join(opt.demo_dir, filename + \"_expert_traj.npy\"), noisy_traj)\n",
    "\n",
    "    # if pl_model_file is not None:\n",
    "    #     np.save(os.path.join(opt.demo_dir, 'preference_learning/' + filename + \"_expert_traj.npy\"), expert_traj)\n",
    "    # else:\n",
    "    #     np.save(os.path.join(opt.demo_dir, filename + \"_expert_traj.npy\"), expert_traj)\n",
    "\n",
    "#     if len(expert_traj_extra) > 0 and pl_model_file is not None:\n",
    "#         expert_traj_extra = np.stack(expert_traj_extra)\n",
    "#         with open(os.path.join(demo_dir, filename + \"_expert_traj_extra.pkl\"), 'wb') as f:\n",
    "#             pickle.dump(expert_traj_extra, f)\n",
    "#         #np.save(os.path.join(opt.demo_dir, filename + \"_expert_traj_extra.npy\"), expert_traj_extra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xml_file': 'hopper_foot_mu1.xml'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_kwargs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env_kwargs[0]\n",
    "model_file = 'ppo_model_600000_steps.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env_kwargs[2]\n",
    "model_file = 'ppo_model_330000_steps.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ppo_model_1000000_steps.zip'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'017'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_name_cost(str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name_cost(name_cost):\n",
    "    name_cost = name_cost.replace('.','')\n",
    "    \n",
    "    return name_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mu3'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_name_string(name_string):\n",
    "    name_string = name_string.replace('{', '_').replace('}', '').replace(' ', '').replace(\"'xml_file'\", '')\n",
    "    name_string = name_string.replace(\"'\", \"\").replace(\":\", \"\").replace('/', '')\n",
    "    name_string = name_string.replace(\".xml\", \"\")\n",
    "    name_string = name_string.replace(\"_hopper_foot_\", \"\")\n",
    "\n",
    "\n",
    "    return name_string\n",
    "format_name_string(str(spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env_kwargs[1]\n",
    "cost = 0.17\n",
    "model_file = 'ppo_model_860000_steps.zip'\n",
    "filename = env_name + format_name_string(str(spec)) +str(cost) + model_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('./demos/preference_learning/' + filename + \"_expert_traj.pkl\"), 'rb') as f:\n",
    "    traj_ccl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demos/suboptimal_demos/hopper/dataset.pkl','rb') as f:\n",
    "    trajs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [np.sum(rewards) for obs,acs,rewards in trajs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [np.sum(rewards) for obs,acs,rewards in traj_ccl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = []\n",
    "model_dir = './demos/suboptimal_demos/hopper/Hopper_intervened'\n",
    "for model_file in os.listdir(model_dir):\n",
    "    if model_file!='.ipynb_checkpoints':\n",
    "        with open(os.path.join(model_dir, model_file), 'rb') as f:\n",
    "            traj_ccl = pickle.load(f)\n",
    "            p = [np.sum(rewards) for obs,acs,rewards in traj_ccl]\n",
    "        V.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[751.8004]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating spurious correlations\n",
    "\n",
    "The generation of the intervened xml files can be done in multiple ways:\n",
    "\n",
    "1. We can intervene on one variable in the file and change the reward multiplier (to create a spurious correlation)</br>\n",
    "2. We can intervene on two variables directly in the file (e.g. damping and friction)</br>\n",
    "3. We can do it through noise</br>\n",
    "\n",
    "Alternatively, rather than intervening on environmental variables, we could consider intervening on e.g. actions, angles etc, as in the paper https://papers.nips.cc/paper/2021/hash/204904e461002b28511d5880e1c36a0f-Abstract.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_sb_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e8770dd320a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hopper-v3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msb_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sb_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msb_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process_sb_args' is not defined"
     ]
    }
   ],
   "source": [
    "# This notebook is used to create the suboptimal trajectories\n",
    "with open('config_sb3.yaml') as parameters:\n",
    "      sb_args = yaml.safe_load(parameters)['Hopper-v3']\n",
    "n_envs = sb_args['n_envs']\n",
    "n_timesteps = int(sb_args['n_timesteps'])\n",
    "policy = sb_args['policy']\n",
    "\n",
    "env_name = 'Hopper-v3'\n",
    "sb_args = process_sb_args(sb_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring option 1 with foot friction and reward\n",
    "# Specifically create a spurious correlation between ctrl_cost_weight and friction. That is the penalty of the hopper taking too large actions.\n",
    "# env_kwargs = [{'xml_file': 'hopper.xml'}, {'xml_file': 'hopper_foot_mu1.xml'}, {'xml_file': 'hopper_foot_mu3.xml'}]\n",
    "env_kwargs = [{'xml_file': 'hopper_foot_mu1.xml'}, {'xml_file': 'hopper_foot_mu3.xml'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'xml_file': 'hopper_foot_mu1.xml'} 0.01\n",
    "{'xml_file': 'hopper_foot_mu1.xml'} 0.05\n",
    "{'xml_file': 'hopper_foot_mu3.xml'} 0.1\n",
    "{'xml_file': 'hopper_foot_mu3.xml'} 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xml_file': 'hopper_foot_mu1.xml'} [0.01, 0.05]\n",
      "{'xml_file': 'hopper_foot_mu3.xml'} [0.1, 0.5]\n"
     ]
    }
   ],
   "source": [
    "for spec, costs in zip(env_kwargs, [[0.01, 0.05],[0.1, 0.5]]):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xml_file': 'hopper_foot_mu1.xml'} 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> SB Training for preference learning using checkpoint callback\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'xml_file': 'hopper_foot_mu1.xml'} 0.05\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> SB Training for preference learning using checkpoint callback\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'xml_file': 'hopper_foot_mu3.xml'} 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> SB Training for preference learning using checkpoint callback\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'xml_file': 'hopper_foot_mu3.xml'} 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> SB Training for preference learning using checkpoint callback\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# default ctrl_cost_weight=0.001 and default foot_mu=2\n",
    "\n",
    "pref_ckpt_dir = './sb_models/ckpt_' + env_name\n",
    "seed = 0\n",
    "\n",
    "if not os.path.exists('demos/preference_learning'):\n",
    "    os.mkdir('demos/preference_learning')\n",
    "\n",
    "def make_env(rank, ctrl_cost):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name, ctrl_cost_weight=ctrl_cost, **spec)\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "#         env = apply_wrappers(env, **wrapper_kwargs)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "for spec, costs in zip(env_kwargs, [[0.01, 0.05],[0.1, 0.5]]):\n",
    "    for cost in costs:\n",
    "        print(spec,cost)\n",
    "        envs = [make_env(i, cost) for i in range(1)]\n",
    "\n",
    "        env = DummyVecEnv(envs)\n",
    "\n",
    "        if not os.path.exists('./sb_models'):\n",
    "            os.mkdir('sb_models')\n",
    "        if not os.path.exists('./sb_models/ckpt'):\n",
    "            os.mkdir('sb_models/ckpt')\n",
    "\n",
    "        log_path = os.path.join('exp_output',\n",
    "                                        'ppo_sb_' + datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "        model = PPOSB(policy, env, **sb_args, tensorboard_log=log_path, verbose=0)\n",
    "\n",
    "        model_filename = \"sb_models/ppo2_\" + env_name + format_name_string(str(spec))+'_ctrl_cost_weight_'+str(cost)\n",
    "\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=pref_ckpt_dir,\n",
    "                                                                 name_prefix='ppo_model')\n",
    "        print(\"-\" * 100)\n",
    "        print(\">>> SB Training for preference learning using checkpoint callback\")\n",
    "        print(\"-\" * 100)\n",
    "        model.learn(total_timesteps=n_timesteps, callback=checkpoint_callback)\n",
    "    #     model = PPOSB(policy, env, **sb_args)#, tensorboard_log=log_path)\n",
    "        model.save(model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative spurious correlation may be to train each file with different frictions and sample at different checkpoints, to create a spurious correlation between the policy and the friction maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xml_file': 'hopper_foot_mu3.xml'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hopper-v3'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_dir = './sb_models/ckpt_' + env_name + format_name_string(str(spec)) + '_' + str(0.1)\n",
    "model = PPOSB.load(os.path.join(model_dir, model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [109.862236] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [255.06236] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [188.74068] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [251.40044] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [224.92062] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [928.04706] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [565.64435] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [150.3659] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [569.2341] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.78784] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [939.89154] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [653.82697] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [931.99506] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [887.81287] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [749.47314] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [822.6898] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [852.064] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [889.1016] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [909.12177] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [883.4743] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.293] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [917.3548] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.75836] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [898.2876] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.39655] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [918.0532] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.87445] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.3165] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.0259] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.9179] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.73346] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.5005] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [916.0332] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [905.9422] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [900.32874] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [918.7353] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [945.1341] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.73193] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [926.51654] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.57654] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.46515] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.0382] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.2166] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.8533] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.00824] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.0042] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.38776] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.40436] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.2566] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.8129] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [736.7834] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.69073] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.23376] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.1235] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.42413] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.71405] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.2023] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [929.3259] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [926.0725] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.4449] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [932.58044] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.06866] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [922.2446] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [908.3941] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [916.76495] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [913.9273] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [914.1789] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [911.8666] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [931.7825] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [920.9006] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.1611] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.3731] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [929.0975] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.0297] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.36285] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.9809] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.72614] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.4462] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.6922] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.69165] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.8486] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.87726] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [971.12006] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [949.94604] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.95447] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [936.42596] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.99713] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.34143] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [939.68414] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.9332] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.5544] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [967.3403] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.8489] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.5466] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.88696] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [961.1997] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [970.4679] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [968.6691] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.137] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.8654] extra threshold 0\n",
      "ppo_model_10000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [101.839645] extra threshold 0\n",
      "ppo_model_20000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [219.23659] extra threshold 0\n",
      "ppo_model_30000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [195.46786] extra threshold 0\n",
      "ppo_model_40000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [182.46938] extra threshold 0\n",
      "ppo_model_50000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [189.51793] extra threshold 0\n",
      "ppo_model_60000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [318.0577] extra threshold 0\n",
      "ppo_model_70000_steps.zip\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: [420.70584] extra threshold 0\n",
      "ppo_model_80000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [247.98029] extra threshold 0\n",
      "ppo_model_90000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [428.13098] extra threshold 0\n",
      "ppo_model_100000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.9289] extra threshold 0\n",
      "ppo_model_110000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.7111] extra threshold 0\n",
      "ppo_model_120000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [734.9468] extra threshold 0\n",
      "ppo_model_130000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.6961] extra threshold 0\n",
      "ppo_model_140000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [886.9149] extra threshold 0\n",
      "ppo_model_150000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [620.904] extra threshold 0\n",
      "ppo_model_160000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [392.07904] extra threshold 0\n",
      "ppo_model_170000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [860.2375] extra threshold 0\n",
      "ppo_model_180000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [887.05304] extra threshold 0\n",
      "ppo_model_190000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [906.8287] extra threshold 0\n",
      "ppo_model_200000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [845.1195] extra threshold 0\n",
      "ppo_model_210000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.0573] extra threshold 0\n",
      "ppo_model_220000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [920.87585] extra threshold 0\n",
      "ppo_model_230000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.36285] extra threshold 0\n",
      "ppo_model_240000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [731.822] extra threshold 0\n",
      "ppo_model_250000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.7197] extra threshold 0\n",
      "ppo_model_260000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [914.50287] extra threshold 0\n",
      "ppo_model_270000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.51013] extra threshold 0\n",
      "ppo_model_280000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.8401] extra threshold 0\n",
      "ppo_model_290000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.0977] extra threshold 0\n",
      "ppo_model_300000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [942.7173] extra threshold 0\n",
      "ppo_model_310000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.62335] extra threshold 0\n",
      "ppo_model_320000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [944.4785] extra threshold 0\n",
      "ppo_model_330000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [911.2698] extra threshold 0\n",
      "ppo_model_340000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [913.6145] extra threshold 0\n",
      "ppo_model_350000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [899.73364] extra threshold 0\n",
      "ppo_model_360000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [922.14026] extra threshold 0\n",
      "ppo_model_370000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.63354] extra threshold 0\n",
      "ppo_model_380000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.22473] extra threshold 0\n",
      "ppo_model_390000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.48517] extra threshold 0\n",
      "ppo_model_400000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.77655] extra threshold 0\n",
      "ppo_model_410000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [948.6597] extra threshold 0\n",
      "ppo_model_420000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.5781] extra threshold 0\n",
      "ppo_model_430000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.74756] extra threshold 0\n",
      "ppo_model_440000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.23615] extra threshold 0\n",
      "ppo_model_450000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [964.00146] extra threshold 0\n",
      "ppo_model_460000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [956.8663] extra threshold 0\n",
      "ppo_model_470000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.3315] extra threshold 0\n",
      "ppo_model_480000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.1828] extra threshold 0\n",
      "ppo_model_490000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.7896] extra threshold 0\n",
      "ppo_model_500000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [960.2473] extra threshold 0\n",
      "ppo_model_510000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [966.18304] extra threshold 0\n",
      "ppo_model_520000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [951.606] extra threshold 0\n",
      "ppo_model_530000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.66156] extra threshold 0\n",
      "ppo_model_540000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.61993] extra threshold 0\n",
      "ppo_model_550000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.2142] extra threshold 0\n",
      "ppo_model_560000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.00464] extra threshold 0\n",
      "ppo_model_570000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [935.90027] extra threshold 0\n",
      "ppo_model_580000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.1527] extra threshold 0\n",
      "ppo_model_590000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.07733] extra threshold 0\n",
      "ppo_model_600000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [926.2326] extra threshold 0\n",
      "ppo_model_610000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [933.4107] extra threshold 0\n",
      "ppo_model_620000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.80695] extra threshold 0\n",
      "ppo_model_630000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [920.1804] extra threshold 0\n",
      "ppo_model_640000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [906.6143] extra threshold 0\n",
      "ppo_model_650000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [917.20105] extra threshold 0\n",
      "ppo_model_660000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [913.6773] extra threshold 0\n",
      "ppo_model_670000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [911.704] extra threshold 0\n",
      "ppo_model_680000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [911.6241] extra threshold 0\n",
      "ppo_model_690000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [929.34735] extra threshold 0\n",
      "ppo_model_700000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [923.27246] extra threshold 0\n",
      "ppo_model_710000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [938.2725] extra threshold 0\n",
      "ppo_model_720000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [927.9383] extra threshold 0\n",
      "ppo_model_730000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [929.9468] extra threshold 0\n",
      "ppo_model_740000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.09784] extra threshold 0\n",
      "ppo_model_750000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [953.7959] extra threshold 0\n",
      "ppo_model_760000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [947.5204] extra threshold 0\n",
      "ppo_model_770000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.178] extra threshold 0\n",
      "ppo_model_780000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.833] extra threshold 0\n",
      "ppo_model_790000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.32404] extra threshold 0\n",
      "ppo_model_800000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [945.72675] extra threshold 0\n",
      "ppo_model_810000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.82764] extra threshold 0\n",
      "ppo_model_820000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.4974] extra threshold 0\n",
      "ppo_model_830000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.70087] extra threshold 0\n",
      "ppo_model_840000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.39246] extra threshold 0\n",
      "ppo_model_850000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [954.2891] extra threshold 0\n",
      "ppo_model_860000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [934.70435] extra threshold 0\n",
      "ppo_model_870000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [930.4906] extra threshold 0\n",
      "ppo_model_880000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [943.7154] extra threshold 0\n",
      "ppo_model_890000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [940.81836] extra threshold 0\n",
      "ppo_model_900000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [952.25116] extra threshold 0\n",
      "ppo_model_910000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [946.9758] extra threshold 0\n",
      "ppo_model_920000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [963.4814] extra threshold 0\n",
      "ppo_model_930000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [961.03436] extra threshold 0\n",
      "ppo_model_940000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [957.4225] extra threshold 0\n",
      "ppo_model_950000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [959.44495] extra threshold 0\n",
      "ppo_model_960000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [958.7187] extra threshold 0\n",
      "ppo_model_970000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.8284] extra threshold 0\n",
      "ppo_model_980000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [969.06165] extra threshold 0\n",
      "ppo_model_990000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [950.65137] extra threshold 0\n",
      "ppo_model_1000000_steps.zip\n",
      "0\n",
      "episode: 0 reward: [955.1054] extra threshold 0\n"
     ]
    }
   ],
   "source": [
    "model_dir = './sb_models/ckpt_' + env_name\n",
    "for spec in env_kwargs:\n",
    "    for model_file in os.listdir(model_dir):\n",
    "        model = PPOSB.load(os.path.join(model_dir, model_file))\n",
    "        save_expert_traj(env, model, spec, pl_model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_hopper_foot_mu1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_name_string(str({'xml_file': 'hopper_foot_mu1.xml'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_demo_files(expert_demo_dir, env_name, spec):\n",
    "    demo_dir = os.listdir(expert_demo_dir)\n",
    "    if spec is not None:\n",
    "        specd_env_name = env_name + format_name_string(str(spec))\n",
    "    else:\n",
    "        specd_env_name = env_name\n",
    "    \n",
    "    demo_files = [f for f in demo_dir if specd_env_name in f]\n",
    "\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    def natural_keys(text):\n",
    "        return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "    demo_files.sort(key=natural_keys)\n",
    "\n",
    "    return demo_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# This notebook is used to create the suboptimal trajectories\n",
     "\n",
     "sb_yml = open(args.sb_config)\n",
     "sb_args = yaml.load(sb_yml)[args.env_name]\n",
     "set_random_seed(args.seed)\n",
     "n_envs = sb_args['n_envs']\n",
     "n_timesteps = int(sb_args['n_timesteps'])\n",
     "policy = sb_args['policy']\n",
     "\n",
     "env_name = args.env_name\n",
     "\n",
     "def save_ranked_expert_demos(opt, model_dir, env_spec):\n",
     "    sb_yml = open(opt.sb_config)\n",
     "    sb_args = yaml.load(sb_yml, Loader=yaml.FullLoader)[opt.env_name]\n",
     "\n",
     "    env_name = opt.env_name\n",
     "    policy = sb_args['policy']\n",
     "    \n",
     "    def without_keys(d, *keys):\n",
     "        return dict(filter(lambda key_value: key_value[0] not in keys, d.items()))\n",
     "\n",
     "    sb_args = without_keys(sb_args, 'n_envs', 'n_timesteps', 'policy',\n",
     "                           'env_wrapper', 'normalize')\n",
     "    \n",
     "    env, tenv = make_venv(opt, 1, env_spec, opt.env_spec_test, {})\n",
     "\n",
     "def make_venv(opt, n_envs, spec, spec_test, wrapper_kwargs, use_rank=True, use_subprocess=False):\n",
     "    def make_evn(rank):\n",
     "        def _thunk():\n",
     "            env = gym.make(args.env_name, **spec)\n",
     "            if use_rank:\n",
     "                seed = args.seed + rank\n",
     "            else:\n",
     "                seed = args.seed\n",
     "            env.seed(seed)\n",
     "            env.action_space.seed(seed)\n",
     "            env.observation_space.seed(seed)\n",
     "            env = apply_wrappers(env, **wrapper_kwargs)\n",
     "            return env\n",
     "\n",
     "        return _thunk\n",
     "    envs = [make_env(i) for i in range(n_envs)]\n",
     "    if use_subprocess:\n",
     "        envs = SubprocVecEnv(envs)\n",
     "    else:\n",
     "        envs = DummyVecEnv(envs)\n",
     "    testing_env = gym.make(opt.env_name, **spec_test)\n",
     "    testing_env.seed(opt.seed)\n",
     "    testing_env.action_space.seed(opt.seed)\n",
     "    testing_env.observation_space.seed(opt.seed)\n",
     "\n",
     "    return envs, testing_env\n",
     "        \n",
     "# spec_kwargs are the various environments to train with\n",
     "env, test_env = make_venv(opt, opt.n_envs, spec_kwargs, opt.env_spec_test, wrapper_kwargs)\n",
     "sb_args = process_sb_args(sb_args)\n",
     "\n",
     "if not os.path.exists('./sb_models'):\n",
     "    os.mkdir('sb_models')\n",
     "if not os.path.exists('./sb_models/ckpt'):\n",
     "    os.mkdir('sb_models/ckpt')\n",
     "\n",
     "log_path = os.path.join(opt.output_dir,\n",
     "                        'ppo_sb_' + datetime.now().strftime('%Y%m%d_%H%M%S') + '_' + opt.exp_id)\n",
     "model = PPOSB(policy, env, **sb_args, tensorboard_log=log_path)\n",
     "\n",
     "model_filename = \"sb_models/ppo2_\" + env_name + format_name_string(\n",
     "    str(spec_kwargs))\n",
     "\n",
     "\n",
     "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=save_checkpoints_for_pl,\n",
     "                                         name_prefix='ppo_model')\n",
     "print(\"-\" * 100)\n",
     "print(\">>> SB Training for preference learning using checkpoint callback\")\n",
     "print(\"-\" * 100)\n",
     "model.learn(total_timesteps=n_timesteps, callback=checkpoint_callback)\n",
     "   \n",
     "model.save(model_filename)\n",
     "\n",
     "save_ranked_expert_demos(opt, save_checkpoints_for_pl, spec_kwargs)\n",
     "\n",
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
